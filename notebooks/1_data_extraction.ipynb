{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# import data extraction support function\n",
    "from src.support.data_extraction_support_draft import extract_table_from_link, extract_productnames_links, extract_categorynames_links, extract_supermarkets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the purpose is to outline and guide in the logical process of extracting the data for the supermarket product price analysis. The goal is to extract, through scraping techniques, historical data of supermarket product prices, divided by different supermarket chains, from three main categories of products: Milk, olive oil and sunflower oil. \n",
    "\n",
    "The main source used for this extraction will be [FACUA](https://super.facua.org/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scraping\n",
    "\n",
    "## 2.1 Get suppermarkets urls to scrape by surface\n",
    "\n",
    "During an initial exploration of the main page of FACUA, buttons quickly appear for every supermarket with available data. The goal is to access those hrefs, if possible, or navigate using those buttons, to be driven to their individual pages.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![surfaces.png](../assets/surfaces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try parsing the main html looking for the hrefs inside those buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful connection.\n"
     ]
    }
   ],
   "source": [
    "link = \"https://super.facua.org\"\n",
    "\n",
    "response = requests.get(link)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successful connection.\")\n",
    "\n",
    "else:\n",
    "    print(\"Connection failed.\")\n",
    "\n",
    "main_soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for the keywords \"Precios en {supermarket}\", hrefs are found rather fast. Therefore, let's extract them that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 supermarket cards.\n"
     ]
    }
   ],
   "source": [
    "supermarket_cards = main_soup.findAll(\"div\",{\"class\":\"card h-100\"})\n",
    "\n",
    "print(f\"There are {len(supermarket_cards)} supermarket cards.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are as many supermarket cards in the parsed html as in the visual exploration of the website. Each cards has the individual hrefs for the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://super.facua.org/mercadona/',\n",
       " 'https://super.facua.org/carrefour/',\n",
       " 'https://super.facua.org/eroski/',\n",
       " 'https://super.facua.org/dia/',\n",
       " 'https://super.facua.org/hipercor/',\n",
       " 'https://super.facua.org/alcampo/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supermarket_links = [card.find(\"a\")[\"href\"] for card in supermarket_cards]\n",
    "supermarket_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the link has been obtained, let's define the process to extract the prices from one supermarket. Then, it will be a matter of replicating it over the remaining 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful connection.\n"
     ]
    }
   ],
   "source": [
    "mercadona_link = supermarket_links[0]\n",
    "\n",
    "response_mercadona = requests.get(mercadona_link)\n",
    "\n",
    "if response_mercadona.status_code == 200:\n",
    "    print(\"Successful connection.\")\n",
    "\n",
    "else:\n",
    "    print(\"Connection failed.\")\n",
    "\n",
    "mercadona_soup = BeautifulSoup(response_mercadona.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Get categories urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the category card html elements inside the supermarket link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 product cards.\n",
      "\n",
      "Product category: Aceite de girasol. Link: https://super.facua.org/mercadona/aceite-de-girasol/\n",
      "Product category: Aceite de oliva. Link: https://super.facua.org/mercadona/aceite-de-oliva/\n",
      "Product category: Leche. Link: https://super.facua.org/mercadona/leche/\n"
     ]
    }
   ],
   "source": [
    "product_category_cards = mercadona_soup.findAll(\"div\",{\"class\":\"card h-100\"})\n",
    "\n",
    "print(f\"There are {len(product_category_cards)} product cards.\\n\")\n",
    "\n",
    "product_category_names = [card.find(\"p\").text.strip() for card in product_category_cards]\n",
    "\n",
    "product_category_links = [card.find(\"a\")[\"href\"] for card in product_category_cards]\n",
    "\n",
    "for name, link in zip(product_category_names, product_category_links):\n",
    "    print(f\"Product category: {name}. Link: {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as with the supermarket, let's just focus on the first url from the categories to later replicate the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful connection.\n"
     ]
    }
   ],
   "source": [
    "first_category_link = product_category_links[0]\n",
    "\n",
    "first_category_link = \"https://super.facua.org/mercadona/aceite-de-girasol/\"\n",
    "\n",
    "response_first_category = requests.get(first_category_link)\n",
    "\n",
    "if response_first_category.status_code == 200:\n",
    "    print(\"Successful connection.\")\n",
    "\n",
    "else:\n",
    "    print(\"Connection failed.\")\n",
    "\n",
    "first_category_soup = BeautifulSoup(response_first_category.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Get products urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the categories, there are again card elements. This time, the card elements hold the urls to each product's information and prices table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 product cards.\n",
      "\n",
      "Product category: Aceite De Girasol Refinado 0,2º Hacendado 1 L.. Link: https://super.facua.org/mercadona/aceite-de-girasol/aceite-de-girasol-refinado-02-hacendado-1-l/\n",
      "Product category: Aceite De Girasol Refinado 0,2º Hacendado 5 L.. Link: https://super.facua.org/mercadona/aceite-de-girasol/aceite-de-girasol-refinado-02-hacendado-5-l/\n"
     ]
    }
   ],
   "source": [
    "product_cards = first_category_soup.findAll(\"div\",{\"class\",\"row gx-4 gx-lg-5 row-cols-2 row-cols-md-3 row-cols-xl-4 justify-content-center\"})[-1]\n",
    "\n",
    "product_cards = product_cards.findAll(\"div\",{\"class\":\"card h-100\"})\n",
    "\n",
    "print(f\"There are {len(product_cards)} product cards.\\n\")\n",
    "\n",
    "product_names = [card.find(\"p\").text.strip() for card in product_cards]\n",
    "\n",
    "product_links = [card.find(\"a\")[\"href\"] for card in product_cards]\n",
    "\n",
    "for name, link in zip(product_names, product_links):\n",
    "    print(f\"Product category: {name}. Link: {link}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again. let's just take one url to parse its html and create the pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful connection.\n"
     ]
    }
   ],
   "source": [
    "first_product_link = product_links[0]\n",
    "\n",
    "response_first_product = requests.get(first_product_link)\n",
    "\n",
    "if response_first_product.status_code == 200:\n",
    "    print(\"Successful connection.\")\n",
    "\n",
    "else:\n",
    "    print(\"Connection failed.\")\n",
    "\n",
    "first_category_soup = BeautifulSoup(response_first_product.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Scrape prices table inside the product page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for a table element inside the product url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 tables.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<table class=\"table table-striped table-responsive text-center\" style=\"width:100%\"><thead><tr><th scope=\"col\">Día</th><th scope=\"col\">Precio (€)</th><th scope=\"col\">Variación</th></tr></thead><tbody><tr><td>12/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>13/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>14/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>15/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>16/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>17/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>18/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>19/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>20/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>21/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>22/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>23/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>24/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>25/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>26/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>27/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>28/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>29/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>30/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>31/07/2024</td><td>1,45</td><td>=</td></tr><tr><td>01/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>02/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>03/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>04/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>05/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>06/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>07/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>08/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>09/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>10/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>11/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>12/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>13/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>14/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>15/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>16/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>17/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>18/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>19/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>20/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>21/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>22/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>23/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>24/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>25/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>26/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>27/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>28/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>29/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>30/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>31/08/2024</td><td>1,45</td><td>=</td></tr><tr><td>01/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>02/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>03/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>04/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>05/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>06/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>07/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>08/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>09/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>10/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>11/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>12/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>13/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>14/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>15/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>16/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>17/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>18/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>19/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>20/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>21/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>22/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>23/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>24/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>25/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>26/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>27/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>28/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>29/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>30/09/2024</td><td>1,45</td><td>=</td></tr><tr><td>01/10/2024</td><td>1,48</td><td style=\"color: green;\">+0,03 (2,07%)</td></tr><tr><td>02/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>03/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>04/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>05/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>06/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>07/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>08/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>09/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>10/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>11/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>12/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>13/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>14/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>15/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>16/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>17/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>18/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>19/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>20/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>21/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>22/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>23/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>24/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>25/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>26/10/2024</td><td>1,48</td><td>=</td></tr><tr><td>27/10/2024</td><td>1,48</td><td>=</td></tr></tbody></table>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = first_category_soup.findAll(\"table\")\n",
    "\n",
    "print(f\"There are {len(tables)} tables.\\n\")\n",
    "\n",
    "product_price_table = tables[0]\n",
    "product_price_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. The table is there and now it's only a matter of getting it's header, to pass it as the column names for the final CSV file, and the table body, for the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Día', 'Precio (€)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_table_head = [element.text.strip() for element in product_price_table.find(\"thead\").findAll(\"th\")][:2]\n",
    "product_table_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12/07/2024', '1,45'],\n",
       " ['13/07/2024', '1,45'],\n",
       " ['14/07/2024', '1,45'],\n",
       " ['15/07/2024', '1,45'],\n",
       " ['16/07/2024', '1,45']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_table_body = [[element.text.strip() for element in row.findAll(\"td\")][:2] for row in product_price_table.find(\"tbody\").findAll(\"tr\")]\n",
    "product_table_body[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the DataFrame from extracting the product's history information would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/07/2024</td>\n",
       "      <td>1,45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13/07/2024</td>\n",
       "      <td>1,45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/07/2024</td>\n",
       "      <td>1,45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15/07/2024</td>\n",
       "      <td>1,45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/07/2024</td>\n",
       "      <td>1,45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>23/10/2024</td>\n",
       "      <td>1,48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>24/10/2024</td>\n",
       "      <td>1,48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>25/10/2024</td>\n",
       "      <td>1,48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>26/10/2024</td>\n",
       "      <td>1,48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>27/10/2024</td>\n",
       "      <td>1,48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1\n",
       "0    12/07/2024  1,45\n",
       "1    13/07/2024  1,45\n",
       "2    14/07/2024  1,45\n",
       "3    15/07/2024  1,45\n",
       "4    16/07/2024  1,45\n",
       "..          ...   ...\n",
       "103  23/10/2024  1,48\n",
       "104  24/10/2024  1,48\n",
       "105  25/10/2024  1,48\n",
       "106  26/10/2024  1,48\n",
       "107  27/10/2024  1,48\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(product_table_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the format that should be used to upload the table to a database with psycopg2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12/07/2024', '1,45', 'supermercado', 'category'),\n",
       " ('13/07/2024', '1,45', 'supermercado', 'category'),\n",
       " ('14/07/2024', '1,45', 'supermercado', 'category')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tuple([row[0], row[1], \"supermercado\",\"category\"]) for row in product_table_body][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the structure repeats along all products, the extraction will follow this pattern as a whole. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Integrated extraction of all products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The functions for extraction can be found in the script `src/support/data_extraction_support_draft.py`. For clarity purposes, the most bottom-level one is written here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_table_from_link(\n",
    "    link: str,\n",
    "    supermarket_name: str,\n",
    "    category_name: str,\n",
    "    product_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts a table from a web page link and returns it as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    link : str\n",
    "        The URL of the web page containing the table to extract.\n",
    "    supermarket_name : str\n",
    "        Name of the supermarket associated with the product.\n",
    "    category_name : str\n",
    "        Category name for the product.\n",
    "    product_name : str\n",
    "        Name of the product.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the extracted table data with columns for \n",
    "        product name, category name, and supermarket name.\n",
    "    \"\"\"\n",
    "\n",
    "    #make request to the specified link\n",
    "    response = requests.get(link)\n",
    "\n",
    "    # check response and proceed if successful\n",
    "    if response.status_code == 200:\n",
    "        #print(\"Successful connection.\")\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Connection failed.\")\n",
    "\n",
    "    #parse html content\n",
    "    product_data_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    #extract table header and body, keeping 2 columns\n",
    "    table_head_list: List[str] = [element.text.strip() for element in product_data_soup.find(\"thead\").findAll(\"th\")][:2]\n",
    "    table_body_list: List[List[str]] = [[element.text.strip() for element in row.findAll(\"td\")][:2] for row in product_data_soup.find(\"tbody\").findAll(\"tr\")]\n",
    "\n",
    "    # convert to dataframe and add column names\n",
    "    extracted_table_df = pd.DataFrame(table_body_list, columns=table_head_list)\n",
    "    extracted_table_df[[\"product_name\", \"category_name\", \"supermarket_name\"]] = product_name, category_name, supermarket_name\n",
    "\n",
    "    return extracted_table_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function above that scrapes the product's information and the other 3 that navigate to get all the urls in the website, this is what the process to extract all product's prices in the website look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_table_from_link() missing 2 required positional arguments: 'category_name' and 'product_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m         product_names, product_links \u001b[38;5;241m=\u001b[39m extract_productnames_links(category_link)\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m product_name, product_link \u001b[38;5;129;01min\u001b[39;00m product_names, product_links:\n\u001b[1;32m---> 15\u001b[0m             product_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_table_from_link\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m             total_result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([total_result_df,product_df])\n\u001b[0;32m     19\u001b[0m total_result_df\n",
      "\u001b[1;31mTypeError\u001b[0m: extract_table_from_link() missing 2 required positional arguments: 'category_name' and 'product_name'"
     ]
    }
   ],
   "source": [
    "total_result_df = pd.DataFrame()\n",
    "\n",
    "supermarket_links = extract_supermarkets(\"https://super.facua.org/\")\n",
    "\n",
    "for supermarket_link in supermarket_links:\n",
    "\n",
    "    category_links = extract_categorynames_links(supermarket_link)\n",
    "\n",
    "    for category_link in category_links:\n",
    "\n",
    "        product_names, product_links = extract_productnames_links(category_link)\n",
    "\n",
    "        for product_name, product_link in product_names, product_links:\n",
    "\n",
    "            product_df = extract_table_from_link(product_link, product_name) # for some reason, this is trying to access an old version of the function\n",
    "\n",
    "            total_result_df = pd.concat([total_result_df,product_df])\n",
    "    \n",
    "total_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process works, but downloading the whole data base, to clean it and the upload it as a whole inside a pipeline can mean that if it fails at any point along the way, nothing will be uploaded. \n",
    "\n",
    "Another proposal is to extract, transform and load on the same product iteration, which can be slower per product, but safer, saving CSV checkpoints along the way. That is what has been done in the updated function `get_table_from_product_link()` inside `src/data_etl.py`, that calls for support function in the final version of extraction supports at `src/support/data_extraction_support.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow to the transformation phase, go to `notebooks/2_data_transformation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-supermercados_analisis",
   "language": "python",
   "name": "my-supermercados_analisis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
